Below is a summary of our findings and comparisons based on our experiments with modified ResNet18 architectures:

---

### **1. Architectural Modifications**

**a. Baseline Modification (for 32×32 inputs):**  
- Replace the 3×3 max pooling with an identity mapping (i.e., remove it).
- Retain the 3×3 kernel with stride 1.  
 A 7×7 kernel on a very small image (e.g., 32×32) oversamples, and the aggressive downsampling from stride 2 and max pooling can discard critical spatial details.
---

### **2. Pretrained vs. Non-Pretrained**

- **Pretrained Models:**  
  - **Advantages:**  
    - They start with weights that capture general features from a large dataset (like ImageNet).  
    - converge faster and often yield higher baseline performance.
  - **Limitations:**  
    - requiring significant fine-tuning for current dataset.

- **Non-Pretrained Models (Training from Scratch):**  
  - **Advantages:**  
    - can be optimized  for the custom dataset.
  - **Limitations:**  
    - They typically require more data and training time to reach a performance level similar to that of a pretrained model.

---

### **3. Effects of Image Size and Kernel Size**

- **Image Size:**  
  - **Large Images (e.g., 224×224):**  
    - Preserve more details and context
    - significant computational cost.  
    - Upscaling small images (e.g., 32×32) to this size does not add information—it only increases computational load.
  - **Small Images (e.g., 32×32):**  
    - Less computationally expensive and more relevant when the original images are of low resolution.  
    - Require architectural adjustments (e.g., smaller kernels, less aggressive pooling) to avoid losing key spatial details.

- **Kernel Size in Conv1:**  
  - **Large Kernels (7×7):**  
    - Provide a large receptive field which is suitable for high-resolution images but can over-smooth small images, losing fine details.  
  - **Smaller Kernels (3×3 or 5×5):**  
    - Better capture local details which are critical for small images.  
    - A 3×3 kernel is computationally cheaper, while a 5×5 may provide a middle ground by capturing slightly broader context without too much overhead.

---

### **4. Explanation of the Observed Differences**

- **Pretrained vs. Non-Pretrained:**  
  - Pretrained networks leverage rich, general features learned from large datasets. However, if the dataset’s image size and content differ significantly from ImageNet (e.g., 32×32 low-resolution images), these features might not align well with the target domain.
  - Non-pretrained models, when optimized for the specific input resolution and domain, can sometimes outperform the transferred features by focusing on the finer details that matter in small images.

- **Impact of Image and Kernel Size:**  
  - **Image Size:** Upscaling a small image to 224×224 does not create new information; it only introduces redundant computation. Optimizing directly for 32×32 inputs is both faster and can lead to better generalization if the model is properly adjusted.
  - **Kernel Size:** A smaller kernel (e.g., 3×3) is more appropriate for small images because it focuses on local patterns without averaging out too much detail. A larger kernel (7×7) may be unnecessary and even detrimental when the image size is small, as it might blend important local features.

- **Computational Efficiency:**  
  - Modifying the initial layers (reducing kernel size, adjusting or removing pooling) not only helps preserve relevant features in small images but also reduces the number of computations required, making the model faster.

