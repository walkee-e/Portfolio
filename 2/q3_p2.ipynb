{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VGGStyleModel:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(512),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        ])\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        return image\n",
    "\n",
    "    def denormalize(self, tensor):\n",
    "        image = tensor.clone().detach().cpu().squeeze(0).numpy().transpose(1, 2, 0)\n",
    "        image = image * self.std + self.mean\n",
    "        return (np.clip(image, 0, 1) * 255).astype(np.uint8)\n",
    "\n",
    "    def get_features(self, image, layers):\n",
    "        features = {}\n",
    "        x = image\n",
    "\n",
    "        for idx, layer in enumerate(self.vgg.children()):\n",
    "            x = layer(x)\n",
    "            if idx in layers:\n",
    "                features[layers[idx]] = x\n",
    "        return features\n",
    "\n",
    "    def gram_matrix(self, tensor):\n",
    "        b, c, h, w = tensor.size()\n",
    "        features = tensor.view(c, h * w)\n",
    "        gram = torch.mm(features, features.t())\n",
    "        return gram.div(c * h * w)\n",
    "\n",
    "    def train(self, content_path, style_path, weights, num_steps=300):\n",
    "\n",
    "        content_img = self.load_image(content_path)\n",
    "        style_img = self.load_image(style_path)\n",
    "        targets = []\n",
    "\n",
    "\n",
    "        for content_weight, style_weight in weights:\n",
    "\n",
    "            target_img = content_img.clone().requires_grad_(True)\n",
    "\n",
    "            layers = {\n",
    "                0: 'conv1_1',\n",
    "                5: 'conv2_1',\n",
    "                10: 'conv3_1',\n",
    "                19: 'conv4_1',\n",
    "                21: 'conv4_2', \n",
    "                28: 'conv5_1'\n",
    "            }\n",
    "            content_layer = 'conv4_2'\n",
    "            style_layers = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "\n",
    "\n",
    "            content_features = self.get_features(content_img, layers)\n",
    "            style_features = self.get_features(style_img, layers)\n",
    "\n",
    "\n",
    "            optimizer = torch.optim.Adam([target_img], lr=0.001)\n",
    "\n",
    "\n",
    "            step = [0]\n",
    "            for _ in tqdm(range(num_steps), desc=\"Training with LBFGS\"):\n",
    "                optimizer.zero_grad()\n",
    "                generated_features = self.get_features(target_img, layers)\n",
    "\n",
    " \n",
    "                content_loss = F.mse_loss(generated_features[content_layer], content_features[content_layer])\n",
    "                \n",
    "            \n",
    "                style_loss = 0\n",
    "                for layer in style_layers:\n",
    "                    gram_gen = self.gram_matrix(generated_features[layer])\n",
    "                    gram_style = self.gram_matrix(style_features[layer])\n",
    "                    style_loss += F.mse_loss(gram_gen, gram_style)\n",
    "\n",
    "                total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "                total_loss.backward(retain_graph=True)\n",
    "                step[0] += 1\n",
    "                optimizer.step()\n",
    "\n",
    "            targets.append(target_img.clone())\n",
    "\n",
    "\n",
    "        for idx, target in enumerate(targets):\n",
    "            result = self.denormalize(target)\n",
    "            Image.fromarray(result).save(f\"generated_image_{idx}.jpg\")\n",
    "\n",
    "\n",
    "model = VGGStyleModel()\n",
    "\n",
    "\n",
    "weights = [\n",
    "    (1e4, 1e2),\n",
    "    (1e3, 1e3),\n",
    "    (1e5, 1e1),\n",
    "    (1e4, 1e4),\n",
    "    (1e2, 1e5)\n",
    "]\n",
    "\n",
    "\n",
    "model.train(\"/home/walke/college/cv/ass2/CV Assignment 2/Q3/content/bear.jpg\", \n",
    "            \"/home/walke/college/cv/ass2/CV Assignment 2/Q3/styles/bet-you.jpg\", \n",
    "            weights, num_steps=300)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
