{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/ai-techsystems/neural-style-transfer-742dca137976\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "content_image  = cv2.imread(\"/home/walke/college/cv/ass2/CV Assignment 2/Q3/content/bear.jpg\")\n",
    "content_image  = cv2.cvtColor(content_image , cv2.COLOR_BGR2RGB)\n",
    "content_image = Image.fromarray(content_image)\n",
    "content_image = transform(content_image).unsqueeze(0)\n",
    "\n",
    "\n",
    "style_image  = cv2.imread(\"/home/walke/college/cv/ass2/CV Assignment 2/Q3/styles/bet-you.jpg\")\n",
    "style_image  = cv2.cvtColor(style_image , cv2.COLOR_BGR2RGB)\n",
    "style_image = Image.fromarray(style_image)\n",
    "style_image = transform(style_image).unsqueeze(0)\n",
    "\n",
    "generated_image = content_image.clone().detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vgg = models.vgg19(pretrained=True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)\n",
    "        \n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "vgg = vgg.to(device)\n",
    "\n",
    "generated_image = generated_image.to(device)\n",
    "content_image = content_image.to(device)\n",
    "style_image = style_image.to(device)\n",
    "    \n",
    "\n",
    "layers = {}\n",
    "conv_count = 1\n",
    "relu_count = 1\n",
    "pool_count = 1\n",
    "    \n",
    "for i, layer in enumerate(vgg.children()):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        name = f'conv_{conv_count}'\n",
    "        conv_count += 1\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        name = f'relu_{relu_count}'\n",
    "        # Replace in-place ReLU\n",
    "        vgg[i] = nn.ReLU(inplace=False)\n",
    "        relu_count += 1\n",
    "    elif isinstance(layer, nn.MaxPool2d):\n",
    "        name = f'pool_{pool_count}'\n",
    "        pool_count += 1\n",
    "    else:\n",
    "        raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
    "        \n",
    "    layers[name] = i\n",
    "    \n",
    "def get_features(image, layers, model=vgg):\n",
    "    features = {}\n",
    "    x = image\n",
    "    \n",
    "    max_layer = max(layers.values())\n",
    "    \n",
    "    for idx, layer in enumerate(model.children()):\n",
    "        x = layer(x)\n",
    "\n",
    "        for name, layer_idx in layers.items():\n",
    "            if idx == layer_idx:\n",
    "                features[name] = x\n",
    "                \n",
    "        if idx >= max_layer:\n",
    "            break\n",
    "                \n",
    "    return features\n",
    "def content_loss(input_features, target_features):\n",
    "    return F.mse_loss(input_features, target_features)\n",
    "\n",
    "\n",
    "def style_loss(input_features, target_features):\n",
    "    input_batch_size, input_channels, input_height, input_width = input_features.size()\n",
    "    input_features = input_features.view(input_batch_size * input_channels, input_height * input_width)\n",
    "    input_gram = torch.mm(input_features, input_features.t())\n",
    "    input_gram = input_gram.div(input_channels * input_height * input_width)\n",
    "\n",
    "    target_batch_size, target_channels, target_height, target_width = target_features.size()\n",
    "    target_features = target_features.view(target_batch_size * target_channels, target_height * target_width)\n",
    "    target_gram = torch.mm(target_features, target_features.t())\n",
    "    target_gram = target_gram.div(target_channels * target_height * target_width)\n",
    " \n",
    "    return F.mse_loss(input_gram, target_gram)\n",
    "\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "    \n",
    "layers = {name: layers[name] for name in set(content_layers + style_layers)}\n",
    "\n",
    "generated_features = get_features(generated_image, layers=layers)\n",
    "content_features = get_features(content_image, layers=layers)\n",
    "style_features = get_features(style_image, layers=layers)\n",
    "\n",
    "# Tcontent_loss = 0\n",
    "# for layer in content_layers:\n",
    "#     Tcontent_loss += content_loss(generated_features[layer], content_features[layer])\n",
    "    \n",
    "# Tstyle_loss = 0\n",
    "# for layer in style_layers:\n",
    "#     Tstyle_loss += style_loss(generated_features[layer], style_features[layer])\n",
    "\n",
    "content_weight = 1e4\n",
    "style_weight = 1e2\n",
    "\n",
    "\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "generated_image = generated_image.clone().detach().requires_grad_(True)  # Ensure it's a leaf tensor\n",
    "\n",
    "optimizer = torch.optim.LBFGS([generated_image])\n",
    "\n",
    "run = [0]\n",
    "while run[0] <= 5:\n",
    "    print(run[0])\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        Tcontent_loss = 0\n",
    "        for layer in content_layers:\n",
    "            Tcontent_loss += content_loss(generated_features[layer], content_features[layer])\n",
    "            \n",
    "        Tstyle_loss = 0\n",
    "        for layer in style_layers:\n",
    "            Tstyle_loss += style_loss(generated_features[layer], style_features[layer])\n",
    "\n",
    "        total_loss = content_weight * Tcontent_loss + style_weight * Tstyle_loss\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        run[0] += 1\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "final_img = generated_image.detach().squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "final_img = (final_img * 255).clip(0, 255).astype('uint8')\n",
    "Image.fromarray(final_img).save(\"/home/walke/college/cv/ass2/CV Assignment 2/LBFGS.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
