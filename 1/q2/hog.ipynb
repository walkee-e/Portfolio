{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import cv2 as cv\n",
    "\n",
    "test_img_labels = pd.read_csv(\"/home/walke/college/cv/ass1/archive/test.csv\")\n",
    "\n",
    "new_test_data = [] \n",
    "\n",
    "for index in range(test_img_labels.shape[0]):\n",
    "    img_csv = np.matrix(test_img_labels.iloc[index, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "    label = test_img_labels.iloc[index, 0]\n",
    "\n",
    "\n",
    "    winSize = (20,20)\n",
    "    blockSize = (10,10)\n",
    "    blockStride = (5,5)\n",
    "    cellSize = (10,10)\n",
    "    nbins = 9\n",
    "    derivAperture = 1\n",
    "    winSigma = -1.\n",
    "    histogramNormType = 0\n",
    "    L2HysThreshold = 0.2\n",
    "    gammaCorrection = 1\n",
    "    nlevels = 64\n",
    "    signedGradients = True\n",
    " \n",
    "    hog = cv.HOGDescriptor(winSize,blockSize,blockStride,\n",
    "    cellSize,nbins,derivAperture,\n",
    "    winSigma,histogramNormType,L2HysThreshold,\n",
    "    gammaCorrection,nlevels, signedGradients)\n",
    "\n",
    "    descriptor = hog.compute(img_csv)\n",
    "\n",
    "    row = np.concatenate(([label], descriptor))\n",
    "    new_test_data.append(row)\n",
    "\n",
    "column_names = [\"label\"] + [f\"feature_{i+1}\" for i in range(len(new_test_data[0]) - 1)]\n",
    "new_df = pd.DataFrame(new_test_data, columns=column_names)\n",
    "\n",
    "\n",
    "new_df.to_csv(\"hog_test.csv\", index=False)\n",
    "\n",
    "train_img_labels = pd.read_csv(\"/home/walke/college/cv/ass1/archive/train.csv\")\n",
    "\n",
    "new_train_data = [] \n",
    "\n",
    "for index in range(train_img_labels.shape[0]):\n",
    "    img_csv = np.matrix(train_img_labels.iloc[index, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "    label = train_img_labels.iloc[index, 0]\n",
    "\n",
    "\n",
    "    winSize = (20,20)\n",
    "    blockSize = (10,10)\n",
    "    blockStride = (5,5)\n",
    "    cellSize = (10,10)\n",
    "    nbins = 9\n",
    "    derivAperture = 1\n",
    "    winSigma = -1.\n",
    "    histogramNormType = 0\n",
    "    L2HysThreshold = 0.2\n",
    "    gammaCorrection = 1\n",
    "    nlevels = 64\n",
    "    signedGradients = True\n",
    " \n",
    "    hog = cv.HOGDescriptor(winSize,blockSize,blockStride,\n",
    "    cellSize,nbins,derivAperture,\n",
    "    winSigma,histogramNormType,L2HysThreshold,\n",
    "    gammaCorrection,nlevels, signedGradients)\n",
    "\n",
    "    descriptor = hog.compute(img_csv)\n",
    "\n",
    "    row = np.concatenate(([label], descriptor))\n",
    "    new_train_data.append(row)\n",
    "\n",
    "column_names = [\"label\"] + [f\"feature_{i+1}\" for i in range(len(new_train_data[0]) - 1)]\n",
    "new_df = pd.DataFrame(new_train_data, columns=column_names)\n",
    "\n",
    "\n",
    "new_df.to_csv(\"hog_train.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hogImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_csv = self.img_labels.iloc[idx, 1:].values\n",
    "        image = img_csv.reshape(9, 9)\n",
    "\n",
    "        label = self.img_labels.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(int(label))\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "training_data_hog = hogImageDataset(\n",
    "    annotations_file = \"hog_train.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_data_hog = hogImageDataset(\n",
    "    annotations_file = \"hog_test.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "train_dataset_hog, val_dataset_hog = torch.utils.data.random_split(training_data_hog, [int(0.75*len(training_data_hog)), int(0.25*len(training_data_hog))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader_hog = DataLoader(train_dataset_hog, batch_size=64, shuffle=True)\n",
    "val_dataloader_hog = DataLoader(val_dataset_hog, batch_size=64, shuffle=True)\n",
    "test_dataloader_hog = DataLoader(test_data_hog, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork_hog(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=81, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 81]) | Values : tensor([[-0.0948, -0.0733, -0.0331,  0.0852,  0.0335,  0.0813,  0.0750, -0.1038,\n",
      "         -0.0309,  0.0683,  0.0234,  0.0170,  0.0881,  0.1013, -0.0531,  0.0334,\n",
      "          0.0232, -0.0358, -0.0420, -0.0741, -0.0854, -0.0515,  0.0475, -0.0406,\n",
      "         -0.0939, -0.0795,  0.0765,  0.0942, -0.0012,  0.0535,  0.1036,  0.0182,\n",
      "          0.0618, -0.0425,  0.0455, -0.0630,  0.1044,  0.0867,  0.0190,  0.0766,\n",
      "         -0.0929, -0.0346, -0.0534, -0.1029, -0.0136,  0.0650,  0.0935, -0.0957,\n",
      "          0.0077,  0.0930,  0.0537,  0.0047,  0.0196,  0.0073, -0.1109, -0.0932,\n",
      "          0.0592, -0.0081,  0.0577,  0.0436,  0.0806,  0.0967,  0.0621, -0.0176,\n",
      "          0.0849, -0.0707, -0.0674,  0.0160, -0.0932,  0.0873,  0.1065, -0.0543,\n",
      "         -0.0012,  0.0908, -0.0012, -0.0795,  0.0777, -0.0607, -0.0982, -0.0913,\n",
      "          0.1097],\n",
      "        [-0.0348, -0.0385, -0.0849, -0.0383, -0.0270,  0.0206,  0.0432, -0.0633,\n",
      "          0.0821, -0.0474,  0.0900,  0.0548, -0.0056, -0.0217, -0.0900, -0.0150,\n",
      "          0.0082,  0.0587,  0.0855,  0.0881, -0.0712,  0.0718,  0.0617, -0.0824,\n",
      "          0.0598, -0.0756,  0.0556,  0.0741,  0.0034,  0.0190,  0.0904,  0.0019,\n",
      "         -0.0624, -0.0831,  0.0303,  0.1008, -0.0746, -0.0234, -0.0819,  0.0944,\n",
      "         -0.1053, -0.0828, -0.0396, -0.0811, -0.0241, -0.0907, -0.0709, -0.0458,\n",
      "         -0.0621,  0.0634, -0.0375,  0.0412, -0.0456, -0.0429, -0.0371,  0.0614,\n",
      "         -0.0247,  0.0835,  0.0804,  0.0789, -0.0947, -0.0620,  0.0794,  0.0772,\n",
      "         -0.0807, -0.0565, -0.1046, -0.0666,  0.0870, -0.1022, -0.0107, -0.0479,\n",
      "          0.0495,  0.0739,  0.0996, -0.0134,  0.0690,  0.0037, -0.0731,  0.0304,\n",
      "          0.0696]], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0346,  0.0093], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0308,  0.0180,  0.0139,  ...,  0.0162,  0.0320, -0.0313],\n",
      "        [ 0.0148,  0.0387,  0.0016,  ..., -0.0296,  0.0299,  0.0415]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0099, -0.0309], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0032,  0.0375,  0.0397,  ..., -0.0415, -0.0273,  0.0430],\n",
      "        [-0.0166, -0.0083, -0.0425,  ..., -0.0154,  0.0253, -0.0021]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0110, -0.0164], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork_hog(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(9*9, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model_hog = NeuralNetwork_hog().to(device)\n",
    "print(model_hog)\n",
    "\n",
    "for name, param in model_hog.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total = len(dataloader)\n",
    "\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader),desc=\"train\", total = total):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device).float(), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, model_path):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).float(), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            y = torch.argmax(y, dim=1)  \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).float(), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            y = torch.argmax(y, dim=1)  \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg Validation loss: {val_loss:>8f} \\n\")\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|          | 0/704 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 73.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 20.2%, Avg Validation loss: 2.284126 \n",
      "\n",
      "Saving Model at epoch: 1\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:11<00:00, 62.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 31.5%, Avg Validation loss: 2.260336 \n",
      "\n",
      "Saving Model at epoch: 2\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:08<00:00, 78.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 40.9%, Avg Validation loss: 2.233301 \n",
      "\n",
      "Saving Model at epoch: 3\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 73.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 45.3%, Avg Validation loss: 2.199994 \n",
      "\n",
      "Saving Model at epoch: 4\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:11<00:00, 62.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 47.3%, Avg Validation loss: 2.156346 \n",
      "\n",
      "Saving Model at epoch: 5\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 76.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 47.9%, Avg Validation loss: 2.098913 \n",
      "\n",
      "Saving Model at epoch: 6\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 75.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 47.6%, Avg Validation loss: 2.026047 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:11<00:00, 61.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 46.7%, Avg Validation loss: 1.937262 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 75.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 48.2%, Avg Validation loss: 1.841416 \n",
      "\n",
      "Saving Model at epoch: 9\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 74.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 50.5%, Avg Validation loss: 1.745885 \n",
      "\n",
      "Saving Model at epoch: 10\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:11<00:00, 63.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 54.0%, Avg Validation loss: 1.658267 \n",
      "\n",
      "Saving Model at epoch: 11\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:08<00:00, 79.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 57.1%, Avg Validation loss: 1.579910 \n",
      "\n",
      "Saving Model at epoch: 12\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 73.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 59.6%, Avg Validation loss: 1.507587 \n",
      "\n",
      "Saving Model at epoch: 13\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 72.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 60.3%, Avg Validation loss: 1.441538 \n",
      "\n",
      "Saving Model at epoch: 14\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 72.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 62.2%, Avg Validation loss: 1.380938 \n",
      "\n",
      "Saving Model at epoch: 15\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:11<00:00, 59.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 63.1%, Avg Validation loss: 1.326199 \n",
      "\n",
      "Saving Model at epoch: 16\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 71.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 64.1%, Avg Validation loss: 1.278082 \n",
      "\n",
      "Saving Model at epoch: 17\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 71.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 64.3%, Avg Validation loss: 1.232260 \n",
      "\n",
      "Saving Model at epoch: 18\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:10<00:00, 64.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 65.7%, Avg Validation loss: 1.193297 \n",
      "\n",
      "Saving Model at epoch: 19\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 704/704 [00:09<00:00, 74.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 66.4%, Avg Validation loss: 1.158844 \n",
      "\n",
      "Saving Model at epoch: 20\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90995/128364945.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 1.158937 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate_hog = 1e-3\n",
    "batch_size_hog = 64\n",
    "epochs_hog = 20\n",
    "\n",
    "loss_fn_hog = nn.CrossEntropyLoss()\n",
    "optimizer_hog = torch.optim.SGD(model_hog.parameters(), lr=learning_rate_hog)\n",
    "\n",
    "# model.to(device)\n",
    "val_accuracy_highest_hog = 0\n",
    "val_accuracy_curr_hog = 0\n",
    "\n",
    "for t in range(epochs_hog):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader_hog, model_hog, loss_fn_hog, optimizer_hog)\n",
    "    val_accuracy_curr_hog = val_loop(test_dataloader_hog, model_hog, loss_fn_hog)\n",
    "    if val_accuracy_curr_hog > val_accuracy_highest_hog:\n",
    "        val_accuracy_highest_hog = val_accuracy_curr_hog\n",
    "        model_path = 'model_1_hog'\n",
    "        print(f\"Saving Model at epoch: {t+1}\\n\")\n",
    "        torch.save(model_hog.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "test_loop(test_dataloader_hog, model_hog, loss_fn_hog, \"model_1_hog\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
