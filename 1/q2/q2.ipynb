{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m             label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[0;32m---> 26\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mCustomImageDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/walke/college/cv/ass1/archive/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m test_data \u001b[38;5;241m=\u001b[39m CustomImageDataset(\n\u001b[1;32m     34\u001b[0m     annotations_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/walke/college/cv/ass1/archive/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     transform\u001b[38;5;241m=\u001b[39mToTensor(),\n\u001b[1;32m     36\u001b[0m     target_transform \u001b[38;5;241m=\u001b[39m Lambda(\u001b[38;5;28;01mlambda\u001b[39;00m y: torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m10\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mscatter_(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, index\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(y), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(training_data, [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.75\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_data)), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.25\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(training_data))])\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mCustomImageDataset.__init__\u001b[0;34m(self, annotations_file, transform, target_transform)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotations_file, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotations_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;241m=\u001b[39m target_transform\n",
      "File \u001b[0;32m~/college/cv/pfizer/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/college/cv/pfizer/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/college/cv/pfizer/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/college/cv/pfizer/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_csv = self.img_labels.iloc[idx, 1:].values\n",
    "        image = img_csv.reshape(28, 28)\n",
    "\n",
    "        label = self.img_labels.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "training_data = CustomImageDataset(\n",
    "    annotations_file = \"/home/walke/college/cv/ass1/archive/train.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_data = CustomImageDataset(\n",
    "    annotations_file = \"/home/walke/college/cv/ass1/archive/test.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(training_data, [int(0.75*len(training_data)), int(0.25*len(training_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0: T-Shirt,\n",
      "    1: Trouser,\n",
      "    2: Pullover,\n",
      "    3: Dress,\n",
      "    4: Coat,\n",
      "    5: Sandal,\n",
      "    6: Shirt,\n",
      "    7: Sneaker,\n",
      "    8: Bag,\n",
      "    9: Ankle Boot\n"
     ]
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    label_new = torch.argmax(label).item()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label_new])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\"\"\n",
    "    0: T-Shirt,\n",
    "    1: Trouser,\n",
    "    2: Pullover,\n",
    "    3: Dress,\n",
    "    4: Coat,\n",
    "    5: Sandal,\n",
    "    6: Shirt,\n",
    "    7: Sneaker,\n",
    "    8: Bag,\n",
    "    9: Ankle Boot\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0327, -0.0188,  0.0145,  ...,  0.0180, -0.0283,  0.0325],\n",
      "        [ 0.0045, -0.0193,  0.0074,  ..., -0.0060, -0.0195,  0.0301]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0317, -0.0052], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0418,  0.0065, -0.0205,  ..., -0.0092, -0.0290, -0.0348],\n",
      "        [-0.0141, -0.0137,  0.0233,  ...,  0.0383,  0.0329,  0.0194]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0014, -0.0328], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0162,  0.0108,  0.0333,  ...,  0.0384, -0.0161,  0.0442],\n",
      "        [-0.0008, -0.0385,  0.0189,  ...,  0.0224, -0.0267,  0.0261]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0038,  0.0180], device='cuda:0', grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    total = len(dataloader)\n",
    "\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in tqdm(enumerate(dataloader),desc=\"train\", total = total):\n",
    "        # Compute prediction and loss\n",
    "        X, y = X.to(device).float(), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, model_path):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).float(), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            y = torch.argmax(y, dim=1)  \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    val_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device).float(), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            y = torch.argmax(y, dim=1)  \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg Validation loss: {val_loss:>8f} \\n\")\n",
    "\n",
    "    return correct\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:10<00:00, 67.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 80.4%, Avg Validation loss: 0.556956 \n",
      "\n",
      "Saving Model at epoch: 1\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:12<00:00, 55.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 85.3%, Avg Validation loss: 0.425879 \n",
      "\n",
      "Saving Model at epoch: 2\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:10<00:00, 65.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 80.6%, Avg Validation loss: 0.670065 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:10<00:00, 67.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 87.0%, Avg Validation loss: 0.375139 \n",
      "\n",
      "Saving Model at epoch: 4\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:12<00:00, 56.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 85.4%, Avg Validation loss: 0.401176 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:11<00:00, 63.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 86.9%, Avg Validation loss: 0.369174 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:14<00:00, 49.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 84.5%, Avg Validation loss: 0.443193 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:11<00:00, 63.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 83.1%, Avg Validation loss: 0.500812 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:12<00:00, 55.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 82.6%, Avg Validation loss: 0.511713 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:12<00:00, 57.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 84.7%, Avg Validation loss: 0.434074 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86510/1745206214.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.374879 \n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# model.to(device)\n",
    "val_accuracy_highest = 0\n",
    "val_accuracy_curr = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_accuracy_curr = val_loop(test_dataloader, model, loss_fn)\n",
    "    if val_accuracy_curr > val_accuracy_highest:\n",
    "        val_accuracy_highest = val_accuracy_curr\n",
    "        model_path = 'model_1'\n",
    "        print(f\"Saving Model at epoch: {t+1}\\n\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "test_loop(test_dataloader, model, loss_fn, \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "test_img_labels = pd.read_csv(\"/home/walke/college/cv/ass1/archive/test.csv\")\n",
    "train_img_labels = pd.read_csv(\"/home/walke/college/cv/ass1/archive/train.csv\")\n",
    "\n",
    "for index in range(test_img_labels.shape[0]):\n",
    "    img_csv = np.matrix(test_img_labels.iloc[index, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "    edges = cv.Canny(img_csv, 100, 200)\n",
    "    test_img_labels.iloc[index, 1:] = edges.flatten()\n",
    "\n",
    "test_img_labels.to_csv(\"edges_test.csv\", index=False)\n",
    "\n",
    "for index in range(train_img_labels.shape[0]):\n",
    "    img_csv = np.matrix(train_img_labels.iloc[index, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "    edges = cv.Canny(img_csv, 100, 200)\n",
    "    train_img_labels.iloc[index, 1:] = edges.flatten()\n",
    "\n",
    "train_img_labels.to_csv(\"edges_train.csv\", index=False)\n",
    "\n",
    "class MLP2ImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_csv = self.img_labels.iloc[idx, 1:].values\n",
    "        image = img_csv.reshape(28, 28)\n",
    "\n",
    "        label = self.img_labels.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "training_data_edge = MLP2ImageDataset(\n",
    "    annotations_file = \"edges_train.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_data_edge = MLP2ImageDataset(\n",
    "    annotations_file = \"edges_test.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "train_dataset_edge, val_dataset_edge = torch.utils.data.random_split(training_data_edge, [int(0.75*len(training_data_edge)), int(0.25*len(training_data_edge))])\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset_edge), size=(1,)).item()\n",
    "    img, label = train_dataset_edge[sample_idx]\n",
    "    label_new = torch.argmax(label).item()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label_new])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "train_dataloader_edge = DataLoader(train_dataset_edge, batch_size=64, shuffle=True)\n",
    "val_dataloader_edge = DataLoader(val_dataset_edge, batch_size=64, shuffle=True)\n",
    "test_dataloader_edge = DataLoader(test_data_edge, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:19<00:00, 36.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 73.6%, Avg Validation loss: 0.727660 \n",
      "\n",
      "Saving Model at epoch: 1\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:14<00:00, 47.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 76.1%, Avg Validation loss: 0.662515 \n",
      "\n",
      "Saving Model at epoch: 2\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:23<00:00, 30.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 79.7%, Avg Validation loss: 0.576568 \n",
      "\n",
      "Saving Model at epoch: 3\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:16<00:00, 42.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 79.6%, Avg Validation loss: 0.591299 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:15<00:00, 45.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 79.3%, Avg Validation loss: 0.617127 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:16<00:00, 42.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 77.5%, Avg Validation loss: 0.711855 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:15<00:00, 46.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 80.9%, Avg Validation loss: 0.581679 \n",
      "\n",
      "Saving Model at epoch: 7\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:16<00:00, 42.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 81.0%, Avg Validation loss: 0.608402 \n",
      "\n",
      "Saving Model at epoch: 8\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:15<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 80.1%, Avg Validation loss: 0.680216 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|| 704/704 [00:15<00:00, 45.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 81.4%, Avg Validation loss: 0.629517 \n",
      "\n",
      "Saving Model at epoch: 10\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84063/1745206214.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.634104 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate_edge = 1e-3\n",
    "batch_size_edge = 64\n",
    "epochs_edge = 10\n",
    "\n",
    "loss_fn_edge = nn.CrossEntropyLoss()\n",
    "optimizer_edge = torch.optim.SGD(model.parameters(), lr=learning_rate_edge)\n",
    "\n",
    "# model.to(device)\n",
    "val_accuracy_highest_edge = 0\n",
    "val_accuracy_curr_edge = 0\n",
    "\n",
    "for t in range(epochs_edge):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader_edge, model, loss_fn_edge, optimizer_edge)\n",
    "    val_accuracy_curr_edge = val_loop(test_dataloader_edge, model, loss_fn_edge)\n",
    "    if val_accuracy_curr_edge > val_accuracy_highest_edge:\n",
    "        val_accuracy_highest_edge = val_accuracy_curr_edge\n",
    "        model_path = 'model_1_edge'\n",
    "        print(f\"Saving Model at epoch: {t+1}\\n\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "test_loop(test_dataloader_edge, model, loss_fn_edge, \"model_1_edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_img_labels = pd.read_csv(\"/home/walke/college/cv/ass1/archive/test.csv\")\n",
    "\n",
    "new_test_data = [] \n",
    "\n",
    "for index in range(test_img_labels.shape[0]):\n",
    "    img_csv = np.matrix(test_img_labels.iloc[index, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "    label = test_img_labels.iloc[index, 0]\n",
    "\n",
    "\n",
    "    winSize = (20,20)\n",
    "    blockSize = (10,10)\n",
    "    blockStride = (5,5)\n",
    "    cellSize = (10,10)\n",
    "    nbins = 9\n",
    "    derivAperture = 1\n",
    "    winSigma = -1.\n",
    "    histogramNormType = 0\n",
    "    L2HysThreshold = 0.2\n",
    "    gammaCorrection = 1\n",
    "    nlevels = 64\n",
    "    signedGradients = True\n",
    " \n",
    "    hog = cv.HOGDescriptor(winSize,blockSize,blockStride,\n",
    "    cellSize,nbins,derivAperture,\n",
    "    winSigma,histogramNormType,L2HysThreshold,\n",
    "    gammaCorrection,nlevels, signedGradients)\n",
    "\n",
    "    descriptor = hog.compute(img_csv)\n",
    "\n",
    "    row = np.concatenate(([label], descriptor))\n",
    "    new_test_data.append(row)\n",
    "\n",
    "column_names = [\"label\"] + [f\"feature_{i+1}\" for i in range(len(new_test_data[0]) - 1)]\n",
    "new_df = pd.DataFrame(new_test_data, columns=column_names)\n",
    "\n",
    "\n",
    "new_df.to_csv(\"hog_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_labels = pd.read_csv(\"/home/walke/college/cv/ass1/archive/train.csv\")\n",
    "\n",
    "new_train_data = [] \n",
    "\n",
    "for index in range(train_img_labels.shape[0]):\n",
    "    img_csv = np.matrix(train_img_labels.iloc[index, 1:].values, dtype=np.uint8).reshape(28, 28)\n",
    "    label = train_img_labels.iloc[index, 0]\n",
    "\n",
    "\n",
    "    winSize = (20,20)\n",
    "    blockSize = (10,10)\n",
    "    blockStride = (5,5)\n",
    "    cellSize = (10,10)\n",
    "    nbins = 9\n",
    "    derivAperture = 1\n",
    "    winSigma = -1.\n",
    "    histogramNormType = 0\n",
    "    L2HysThreshold = 0.2\n",
    "    gammaCorrection = 1\n",
    "    nlevels = 64\n",
    "    signedGradients = True\n",
    " \n",
    "    hog = cv.HOGDescriptor(winSize,blockSize,blockStride,\n",
    "    cellSize,nbins,derivAperture,\n",
    "    winSigma,histogramNormType,L2HysThreshold,\n",
    "    gammaCorrection,nlevels, signedGradients)\n",
    "\n",
    "    descriptor = hog.compute(img_csv)\n",
    "\n",
    "    row = np.concatenate(([label], descriptor))\n",
    "    new_train_data.append(row)\n",
    "\n",
    "column_names = [\"label\"] + [f\"feature_{i+1}\" for i in range(len(new_train_data[0]) - 1)]\n",
    "new_df = pd.DataFrame(new_train_data, columns=column_names)\n",
    "\n",
    "\n",
    "new_df.to_csv(\"hog_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhogImageDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, annotations_file, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(annotations_file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class hogImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_csv = self.img_labels.iloc[idx, 1:].values\n",
    "        image = img_csv.reshape(9, 9)\n",
    "\n",
    "        label = self.img_labels.iloc[idx, 0]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(int(label))\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "training_data_hog = hogImageDataset(\n",
    "    annotations_file = \"hog_train.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "test_data_hog = hogImageDataset(\n",
    "    annotations_file = \"hog_test.csv\",\n",
    "    transform=ToTensor(),\n",
    "    target_transform = Lambda(lambda y: torch.zeros(\n",
    "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))\n",
    ")\n",
    "\n",
    "train_dataset_hog, val_dataset_hog = torch.utils.data.random_split(training_data_hog, [int(0.75*len(training_data_hog)), int(0.25*len(training_data_hog))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "\n",
    "    sample_idx = torch.randint(len(train_dataset_hog), size=(1,)).item()\n",
    "\n",
    "    img, label = train_dataset_hog[sample_idx]\n",
    "\n",
    "    label_new = torch.argmax(label).item()\n",
    "\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "\n",
    "    plt.title(labels_map[label_new])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "train_dataloader_hog = DataLoader(train_dataset_hog, batch_size=64, shuffle=True)\n",
    "val_dataloader_hog = DataLoader(val_dataset_hog, batch_size=64, shuffle=True)\n",
    "test_dataloader_hog = DataLoader(test_data_hog, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_hog(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(9*9, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "model_hog = NeuralNetwork_hog().to(device)\n",
    "print(model_hog)\n",
    "\n",
    "for name, param in model_hog.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_hog = 1e-3\n",
    "batch_size_hog = 64\n",
    "epochs_hog = 20\n",
    "\n",
    "loss_fn_hog = nn.CrossEntropyLoss()\n",
    "optimizer_hog = torch.optim.SGD(model_hog.parameters(), lr=learning_rate_hog)\n",
    "\n",
    "# model.to(device)\n",
    "val_accuracy_highest_hog = 0\n",
    "val_accuracy_curr_hog = 0\n",
    "\n",
    "for t in range(epochs_hog):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader_hog, model_hog, loss_fn_hog, optimizer_hog)\n",
    "    val_accuracy_curr_hog = val_loop(test_dataloader_hog, model_hog, loss_fn_hog)\n",
    "    if val_accuracy_curr_hog > val_accuracy_highest_hog:\n",
    "        val_accuracy_highest_hog = val_accuracy_curr_hog\n",
    "        model_path = 'model_1_hog'\n",
    "        print(f\"Saving Model at epoch: {t+1}\\n\")\n",
    "        torch.save(model_hog.state_dict(), model_path)\n",
    "\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "test_loop(test_dataloader_hog, model_hog, loss_fn_hog, \"model_1_hog\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pfizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
